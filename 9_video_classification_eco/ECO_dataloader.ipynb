{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kinetics 동영상 데이터셋을 데이터로더로 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kinetics 동영상 데이터셋으로 ECO 데이터로더작성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/kinetics_videos/bungee jumping/40c7413c-cda1-4e7c-bc53-3b5dc44db082\n",
      "./data/kinetics_videos/bungee jumping/a9bdfaa1-81de-4bbb-8aac-473c79c1096f\n"
     ]
    }
   ],
   "source": [
    "def make_datapath_list(root_path):\n",
    "    video_list = list()\n",
    "\n",
    "    class_list = os.listdir(root_path)\n",
    "\n",
    "    for class_list_i in (class_list):\n",
    "        class_path = os.path.join(root_path, class_list_i)\n",
    "\n",
    "        for file_name in os.listdir(class_path):\n",
    "            name, ext = os.path.splitext(file_name)\n",
    "\n",
    "            if ext == \".mp4\":\n",
    "                continue\n",
    "            \n",
    "            video_img_directory_path = os.path.join(class_path, name)\n",
    "            video_list.append(video_img_directory_path)\n",
    "\n",
    "    return video_list\n",
    "\n",
    "root_path = \"./data/kinetics_videos/\"\n",
    "video_list = make_datapath_list(root_path)\n",
    "print(video_list[0])\n",
    "print(video_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoTransform():\n",
    "    def __init__(self, resize, crop_size, mean, std):\n",
    "        self.data_transform = {\n",
    "            \"train\":torchvision.transforms.Compose([\n",
    "                GroupResize(int(resize)),\n",
    "                GroupCenterCrop(crop_size),\n",
    "                GroupToTensor(),\n",
    "                GroupImgNormalize(mean, std),\n",
    "                Stack()\n",
    "            ]),\n",
    "            \"val\":torchvision.transforms.Compose([\n",
    "                GroupResize(int(resize)),\n",
    "                GroupCenterCrop(crop_size),\n",
    "                GroupToTensor(),\n",
    "                GroupImgNormalize(mean, std),\n",
    "                Stack()\n",
    "            ])\n",
    "        }\n",
    "    \n",
    "    def __call_(self, img_group, phase):\n",
    "        return self.data_transform[phase](img_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupResize():\n",
    "    def __init__(self, resize, interpolation=Image.BILINEAR):\n",
    "        self.rescaler = torchvision.transforms.Resize(resize, interpolation)\n",
    "\n",
    "    def __call__(self, img_group):\n",
    "        return [self.rescaler(img) for img in img_group]\n",
    "\n",
    "class GroupCenterCrop():\n",
    "    def __init__(self, crop_size):\n",
    "        self.ccrop = torchvision.transforms.CenterCrop(crop_size)\n",
    "\n",
    "    def __call__(self, img_group):\n",
    "        return [self.ccrop(img) for img in img_group]\n",
    "\n",
    "class GroupToTensor():\n",
    "    def __init__(self):\n",
    "        self.to_tensor = torchvision.transforms.ToTensor()\n",
    "\n",
    "    def __call__(self, img_group):\n",
    "        return [self.to_tensor(img)*255 for img in img_group]\n",
    "\n",
    "class GroupImgNormalize():\n",
    "    def __init__(self, mean, std):\n",
    "        self.normalize = torchvision.transforms.Normalize(mean, std)\n",
    "\n",
    "    def __call__(self, img_group):\n",
    "        return [self.normalize(img) for img in img_group]\n",
    "\n",
    "class Stack():\n",
    "    def __call__(self, img_group):\n",
    "        ret = torch.cat([(x.filp(dims=[0])).unsqueeze(dim=0) for x in img_group], dim=0)\n",
    "\n",
    "        return ret"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
